import os
import torch
from datasets import load_dataset
import numpy as np
import gc
import tqdm
import json
import pickle
from transformers import set_seed
from batched_specdec import speculative_generate_batch
from batched_specdec.logits_processor import NucleusProcessor, GreedyProcessor
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse

SEED = 42
set_seed(SEED)

# load config from args. if none, default to file
parser = argparse.ArgumentParser()
parser.add_argument('--config', type=str, default=None)
args = parser.parse_args()

if args.config is not None:
    with open(args.config, 'r') as f:
        config = json.load(f)
else:
    config = {
        'dir': './results/gsm-qwen-1000/',
        'num_prompts': 1000,
        'gen_len': 100,
        'gamma': 5,
        'logits_processor': {
            'type': 'NucleusProcessor',
            'temperature': 0.6,
            'top_p': 0.95
        },
        'dataset_name': 'openai/gsm8k',
        'models': {
            'target': 'Qwen/Qwen3-4B-Instruct-2507',
            'drafts': {
                'base': 'Qwen/Qwen3-0.6B',
            }
        },
        'batch_size': 40,
        'show_output': True,
    }

if not os.path.exists(config['dir']):
    os.makedirs(config['dir'])

NUM_PROMPTS = config['num_prompts']  # Number of prompts to evaluate
gen_len = config['gen_len']       # Maximum number of tokens generated (could over pass when using speculative decoding)
gamma = config['gamma']          # Number of drafts generated by the drafter model at each step
if config['logits_processor']['type'] == 'NucleusProcessor':
    logits_processor = NucleusProcessor(temperature=config['logits_processor']['temperature'], top_p=config['logits_processor']['top_p'])
else:
    logits_processor = GreedyProcessor()

# Load dataset
if config.get('dataset_name') == 'openai/gsm8k':
    dataset = load_dataset("openai/gsm8k", "main", split='test').shuffle(seed=SEED)
else:
    dataset = load_dataset("rishabhrj11/cnn_dailymail_512", split='test').shuffle(seed=SEED)

dataset=dataset.select(list(range(NUM_PROMPTS)))

target=config['models']['target']
tokenizer = AutoTokenizer.from_pretrained(target)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def map_to_prompts(example):
    if config.get('dataset_name') == 'openai/gsm8k':
        message= [
            {"role": "system", "content": "In math word problem given by the user, reason step by step and put your final answer within \boxed{}"},
            {"role": "user", "content": example["question"]}
        ]
    else:
        message= [
            {"role": "system", "content": "Write a very short summary for the user's article."},
            {"role": "user", "content": example["article"]}
        ]
    return {'prompt':tokenizer.apply_chat_template(
        message,
        tokenize=False,
        add_generation_prompt=True
    )}

dataset = dataset.map(map_to_prompts)
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
target_model = AutoModelForCausalLM.from_pretrained(target, device_map=device, dtype=torch.bfloat16)
target_model.eval()

def evaluate_generation(draft_model, prompts, max_new_tokens, output_file=None):
    speculative_results = []
    alphas = []
    outputs = []
    batch_size = config['batch_size']

    for start_idx in tqdm.tqdm(range(0, len(prompts), batch_size), total=(len(prompts)+batch_size-1)//batch_size):
        end_idx = min(start_idx + batch_size, len(prompts))
        batch_prompts = prompts[start_idx:end_idx]['prompt']
        print(f"Evaluating prompts {start_idx} to {end_idx-1}...")
        input_ids = tokenizer(batch_prompts, max_length=1024, truncation=True).input_ids
        output_ids_sd, alpha, stats = speculative_generate_batch(
                input_ids,
                draft_model,
                target_model,
                logits_processor=logits_processor,
                gamma=gamma,
                max_gen_len=max_new_tokens,
                eos_tokens_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
                collect_stats=True, 
                tokenizer=tokenizer,
                debug=config.get('show_output', True)
            )
        print("Acceptance rate:", np.mean(alpha))
        speculative_results.extend(stats)
        alphas.extend(alpha)
        outputs.extend(output_ids_sd)
        
        # Free memory after each batch
        del input_ids
        gc.collect()  # Force Python garbage collection first
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        elif torch.backends.mps.is_available():
            torch.mps.empty_cache()
            torch.mps.synchronize()

    return speculative_results, alphas, outputs

for draft_name, draft in config['models']['drafts'].items():
    print(f"Evaluating draft model: {draft_name}")
    draft_model  = AutoModelForCausalLM.from_pretrained(draft, device_map=device, dtype=torch.bfloat16)
    draft_model.eval()
    draft_model.config.use_cache = False  # Disable internal caching
    results, alphas, outputs = evaluate_generation(draft_model, dataset, gen_len)
    with open(os.path.join(config['dir'], f'{draft_name}-stats.pkl'), 'wb') as f:
        pickle.dump(results, f)
    with open(os.path.join(config['dir'], f'{draft_name}-ouputs.pkl'), 'wb') as f:
        pickle.dump(outputs, f)
    print(f'Alpha for {draft_name}:', np.mean(alphas))
    print(f"Saved {config['dir']}/{draft_name}")
    config[f'results_{draft_name}'] = {
        f'alpha': float(np.mean(alphas)),
    }
    del outputs
    del draft_model
    gc.collect()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()

with open(os.path.join(config['dir'], 'config.json'), 'w') as f:
    json.dump(config, f, indent=4)