import torch
from datasets import load_dataset
import numpy as np
import os
import tqdm
import json
import pickle
from transformers import set_seed
from batched_specdec import speculative_generate_batch
from batched_specdec.logits_processor import NucleusProcessor, GreedyProcessor
from transformers import AutoTokenizer, AutoModelForCausalLM

SEED = 42
set_seed(SEED)

config = {
    'dir': './results/gsm-qwen-1000/',
    'num_prompts': 1000,
    'gen_len': 312,
    'gamma': 5,
    'logits_processor': {
        'type': 'NucleusProcessor',
        'temperature': 0.6,
        'top_p': 0.95
    },
    'dataset_name': 'openai/gsm8k',
    # 'quantize_teacher': True,
    'models': {
        'target': 'Qwen/Qwen3-4B-Instruct-2507',
        'drafts': {
            'base': 'Qwen/Qwen3-0.6B',
        }
    }
}

DEBUG=0

if not os.path.exists(config['dir']):
    os.makedirs(config['dir'])

NUM_PROMPTS = config['num_prompts']  # Number of prompts to evaluate
gen_len = config['gen_len']       # Maximum number of tokens generated (could over pass when using speculative decoding)
gamma = config['gamma']          # Number of drafts generated by the drafter model at each step
if config['logits_processor']['type'] == 'NucleusProcessor':
    logits_processor = NucleusProcessor(temperature=config['logits_processor']['temperature'], top_p=config['logits_processor']['top_p'])
else:
    logits_processor = GreedyProcessor()

# Load dataset
if config.get('dataset_name') == 'openai/gsm8k':
    dataset = load_dataset("openai/gsm8k", "main", split='test').shuffle(seed=SEED) # First 50 examples for quick eval
else:
    dataset = load_dataset("rishabhrj11/cnn_dailymail_512", split='test').shuffle(seed=SEED) # 1% for demo; remove slice for full run

dataset=dataset.select(list(range(50, 50+NUM_PROMPTS)))

target=config['models']['target']
tokenizer = AutoTokenizer.from_pretrained(target)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def map_to_prompts(example):
    if config.get('dataset_name') == 'openai/gsm8k':
        message= [
            {"role": "system", "content": "In math word problem given by the user, reason step by step and put your final answer within \boxed{}"},
            {"role": "user", "content": example["question"]}
        ]
    else:
        message= [
            {"role": "system", "content": "Write a very short summary for the user's article."},
            {"role": "user", "content": example["article"]}
        ]
    return {'prompt':tokenizer.apply_chat_template(
        message,
        tokenize=False,
        add_generation_prompt=True
    )}

dataset = dataset.map(map_to_prompts)
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu")
target_model = AutoModelForCausalLM.from_pretrained(target, device_map=device, torch_dtype="auto")
target_model.eval()

def evaluate_generation(draft_model, prompts, max_new_tokens):
    speculative_results = []
    alphas = []
    outputs = []
    batch = 2

    for start_idx in tqdm.tqdm(range(0, len(prompts), batch), total=(len(prompts)+batch-1)//batch):
        end_idx = min(start_idx + batch, len(prompts))
        batch_prompts = prompts[start_idx:end_idx]['prompt']
        print(f"Evaluating prompts {start_idx} to {end_idx-1}...")
        input_ids = []
        for prompt in batch_prompts:
            input_ids.append(tokenizer([prompt], return_tensors="pt", max_length=1024, truncation=True).input_ids[0].tolist())
        output_ids_sd, alpha, stats = speculative_generate_batch(
                input_ids,
                draft_model,
                target_model,
                logits_processor=logits_processor,
                gamma=gamma,
                max_gen_len=max_new_tokens,
                eos_tokens_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id,
                collect_stats=True
            )
        print("Acceptance rate:", np.mean(alpha))
        speculative_results.extend(stats)
        alphas.extend(alpha)
        outputs.extend(output_ids_sd)

    return speculative_results, alphas, outputs

for draft_name, draft in config['models']['drafts'].items():
    print(f"Evaluating draft model: {draft_name}")
    draft_model  = AutoModelForCausalLM.from_pretrained(draft, device_map=device, torch_dtype="auto")
    draft_model.eval()
    results, alphas, outputs = evaluate_generation(draft_model, dataset, gen_len)
    with open(os.path.join(config['dir'], f'{draft_name}-stats.pkl'), 'wb') as f:
        pickle.dump(results, f)
    with open(os.path.join(config['dir'], f'{draft_name}-ouputs.pkl'), 'wb') as f:
        pickle.dump(outputs, f)
    print(f'Alpha for {draft_name}:', np.mean(alphas))
    print(f"Saved {config['dir']}/{draft_name}")
    config[f'results_{draft_name}'] = {
        f'alpha': float(np.mean(alphas)),
    }
    del outputs
    del draft_model

with open(os.path.join(config['dir'], 'config.json'), 'w') as f:
    json.dump(config, f, indent=4)